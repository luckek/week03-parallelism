{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Notes from Doug\n",
    "\n",
    "If people can take one lesson away from reading this, itâ€™s why do we use GPUs and how is the computational model associated with GPUs more sensible for NN needs? \n",
    "Sections 5-7 are the real important ones\n",
    "What parts of NN are able to be parallelized? SIMD, sending batches\n",
    "\n",
    "\n",
    "Set the stage for GPUs: \n",
    "What are they, how do they work? \n",
    "Why are they good for matrix multiplication?\n",
    "How is that useful for machine learning? \n",
    "How can convolution/recurrent stuff be reshaped to be used in this way? \n",
    "\n",
    "\n",
    "Section 8 can be mined for discussion topics \n",
    "What are limits and why - not enough data, or not enough computational time?\n",
    "What key advances will be necessary to use larger data sets? \n",
    "OR what will make training high-performance models available to someone on their desktop? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5 Deep Neural Networks(DNN)\n",
    "\n",
    "Current trend if toward concurrency within mini-batches\n",
    " \n",
    "Even if SIMD is not explicitly utilized, your code is almost garunteed to use SIMD\n",
    "\n",
    "GPUs accel at performing matrix and vector operations \n",
    "\n",
    "Three periods in the history of classification neural networks:\n",
    "  1. Experimentation(~1985-2010) \n",
    "  2. Growth(2010-2015)\n",
    "  3. Resource Conservation(2015-present) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6 Concurrency In Networks\n",
    "\n",
    "![Parallelism schemes: Data, Model, and Pipeline](img/par_Types.PNG)\n",
    "\n",
    "\n",
    "Work-Depth model:\n",
    "  Work: amount of actual computation required for a specific operator\n",
    "  Depth: longest chain of of data dependencies\n",
    "  Lower depth operations are more trivial to parallelize\n",
    "\n",
    "![Work-Depth model](img/tab_4.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.1 Data Parallelism\n",
    "  Since Most operators are independant with respect to the sample size, a single operator can be performed concurrently on different batches of the input data.\n",
    "  Parallel computation does not neet to synchronize after every operation.\n",
    "\n",
    "![data level parallelism](img/data_par.png)\n",
    "\n",
    "How It's Done\n",
    "  Partition*N* samples into minibatches\n",
    "  Distrubute batches across computational resources\n",
    "\n",
    "\n",
    "Scaling\n",
    " Parallel scaling is determined by the minibatch size\n",
    "\n",
    "Batch Normalization(BN)\n",
    "  Normalizing a batch requries synchronization across the batch.\n",
    "  Number of times BN is required varies between architectures.\n",
    "  Snce BN requires synchronization of all samples a minibatch, if minibatches are small enough to be handled independently, no additional synchronization is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.2 Model Parallelism\n",
    "\n",
    "Model parallelism means to split portions of individual layers such that they can be computed independently.\n",
    " Critical in situations where the entire model does not fit in memory.\n",
    "\n",
    "Unline data parallelism, sample batch is distributed to all processors.\n",
    "\n",
    "Since an individual layer is broken up between processors, the inter-layer data depenedencies require communication between the processors.\n",
    "  e.g., a network comprised of 2 fully-connected layers, parallelized between 4 processors. Each processor computes 1/4th of the nodes in each layer,and then sends the results to the other 3 processors.\n",
    "\n",
    "The amount of data dependencies determine overall performance.\n",
    "*Convolutional Layer > Fully-connected Layer > Locally Connected Layer*\n",
    "\n",
    "In case of DNNs, partition work according to dimension (i.e. C, H or W for a 4D tensor). The dependencies which in turn generate communication determines overall performance. Fully connected layers incur all-to-all. Has been proposed to mitigate this problem by making processor responsible for 2x number of nodes, with overlap. Therefore more computation but less communicantion\n",
    "\n",
    "Convoloutional layers require a large amount of communication. In comparison, Locally connected networks need very little communication, and are an excellent choice for model parallelism\n",
    "\n",
    "\n",
    "\n",
    "Note: Should probably (briefly)talk about tree nets as they are analagous to ensamble classifiers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.3 Pipelining\n",
    "\n",
    "Partitioning the DNN based on depth\n",
    "Each processor comptues one or more layers of the DNN, and streams the results to the next layer on another processor.\n",
    "\n",
    "Advantages:\n",
    "  Each processor only needs parameters relevant to their own slice of the NN\n",
    "  Communication between processors is fixed; message source and destination are always known\n",
    "\n",
    "Disadvantages:\n",
    "  Pipeline bottnecks\n",
    "  Latency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 7 Concurrency In Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7.1 Model Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7.2 Parameter Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7.3 Training Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 8. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 8.1 Parameter Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 8.2 Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}