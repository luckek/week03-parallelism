{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Single Machine Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 SIMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Mutli-Threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multi Machine Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parallel Algortihms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 MPI *allreduce*\n",
    "\n",
    "Used to sum large tables of *m* independent parameters.\n",
    "\n",
    "Commonly used operation in deep learning\n",
    "\n",
    "Several algoithms exist, optimal one depends on system, number of processes and message size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Effeciency Trade Off: Generalization vs. Utilization\n",
    "\n",
    "Small minibatches are not able to harness the inherent concurrency in evaluation of the loss functions (i.e. they underutilie the hardware)\n",
    "\n",
    "Large minibatches lead to poor generalization (high validation error) \n",
    "\n",
    "Large minibatches converge and generalize well only when:\n",
    "  a. learning rates are adjusted statistically or adaptively\n",
    "  b. using a \"warmup\" phase\n",
    "  c. using the minibatch size to control gradient variance\n",
    "  d. adaptively increasing minibatch size during training\n",
    "  e. when using specific learning rate schedules\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Deep Neural Networks(DNN)\n",
    "\n",
    "Current trend if toward concurrency within mini-batches\n",
    " \n",
    "Even if SIMD is not explicitly utilized, your code is almost garunteed to use SIMD\n",
    "\n",
    "GPUs accel at performing matrix and vector operations \n",
    "\n",
    "Three periods in the history of classification neural networks:\n",
    "  1. Experimentation(~1985-2010) \n",
    "  2. Growth(2010-2015)\n",
    "  3. Resource Conservation(2015-present) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Concurrency In Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Data Parallelism\n",
    "\n",
    "Pattern Parallelism\n",
    "  Partition *N* samples into minibatches\n",
    "  Distrubute batches across computational resources\n",
    "  \n",
    "Batch Normalization(BN)\n",
    "  Can normalize within minibatches\n",
    "  Or, use Weight Normalization(WN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Model Parallelism\n",
    "\n",
    "In case of DNNs, partition work according to dimension (i.e. C, H or W for a 4D tensor). The dependencies which in turn generate communication determines overall performance. Fully connected layers incur all-to-all. Has been proposed to mitigate this problem by making processor responsible for 2x number of nodes. Therefore more computation but less communicantion\n",
    "\n",
    "In case of CNNs, not as easy. Can use LCNs instead\n",
    "\n",
    "TODO: exapand on this\n",
    "\n",
    "TODO: tree nets?\n",
    "\n",
    "TODO: include / make graphics similiar to figure 14\n",
    "\n",
    "Note: Should probably (briefly)talk about tree nets as they are analagous to ensamble classifiers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Pipelining\n",
    "\n",
    "Overlaping communications \n",
    "\n",
    "OR\n",
    "\n",
    "paritioning of DNN based on depth, assigning layers to specific processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Concurrency In Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Model Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Parameter Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Training Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Parameter Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
