{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from Doug\n",
    "\n",
    "If people can take one lesson away from reading this, itâ€™s why do we use GPUs and how is the computational model associated with GPUs more sensible for NN needs? \n",
    "Sections 5-7 are the real important ones\n",
    "What parts of NN are able to be parallelized? SIMD, sending batches\n",
    "\n",
    "\n",
    "Set the stage for GPUs: \n",
    "What are they, how do they work? \n",
    "Why are they good for matrix multiplication?\n",
    "How is that useful for machine learning? \n",
    "How can convolution/recurrent stuff be reshaped to be used in this way? \n",
    "\n",
    "\n",
    "Section 8 can be mined for discussion topics \n",
    "What are limits and why - not enough data, or not enough computational time?\n",
    "What key advances will be necessary to use larger data sets? \n",
    "OR what will make training high-performance models available to someone on their desktop? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Deep Neural Networks(DNN)\n",
    "\n",
    "Current trend if toward concurrency within mini-batches\n",
    " \n",
    "Even if SIMD is not explicitly utilized, your code is almost garunteed to use SIMD\n",
    "\n",
    "GPUs accel at performing matrix and vector operations \n",
    "\n",
    "Three periods in the history of classification neural networks:\n",
    "  1. Experimentation(~1985-2010) \n",
    "  2. Growth(2010-2015)\n",
    "  3. Resource Conservation(2015-present) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Concurrency In Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Data Parallelism\n",
    "\n",
    "Pattern Parallelism\n",
    "  Partition *N* samples into minibatches\n",
    "  Distrubute batches across computational resources\n",
    "  \n",
    "Batch Normalization(BN)\n",
    "  Can normalize within minibatches\n",
    "  Or, use Weight Normalization(WN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Model Parallelism\n",
    "\n",
    "In case of DNNs, partition work according to dimension (i.e. C, H or W for a 4D tensor). The dependencies which in turn generate communication determines overall performance. Fully connected layers incur all-to-all. Has been proposed to mitigate this problem by making processor responsible for 2x number of nodes. Therefore more computation but less communicantion\n",
    "\n",
    "In case of CNNs, not as easy. Can use LCNs instead\n",
    "\n",
    "TODO: exapand on this\n",
    "\n",
    "TODO: tree nets?\n",
    "\n",
    "TODO: include / make graphics similiar to figure 14\n",
    "\n",
    "Note: Should probably (briefly)talk about tree nets as they are analagous to ensamble classifiers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Pipelining\n",
    "\n",
    "Overlaping communications \n",
    "\n",
    "OR\n",
    "\n",
    "paritioning of DNN based on depth, assigning layers to specific processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Concurrency In Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Model Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Parameter Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Training Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Parameter Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
